{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxhJXClZfask"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wul_jMrMfel0"
   },
   "source": [
    "## resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VW8AeCgBmwoM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from google.colab import drive\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import Counter\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6RCbCFNm1Wq",
    "outputId": "ea33aa03-8916-4314-cdeb-40cc4141fd4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "FILE_PATH = \"/content/drive/MyDrive/Colab Notebooks/해양 침적 쓰레기 이미지/Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcU10E3GnTvE"
   },
   "outputs": [],
   "source": [
    "os.chdir(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kltu8mtjfZM9"
   },
   "outputs": [],
   "source": [
    "def resize_images(image_dir, output_dir, size):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    images = os.listdir(image_dir)\n",
    "    num_images = len(images)\n",
    "\n",
    "    for i, image in enumerate(images[:5]):\n",
    "        os.path.join(image_dir, image)\n",
    "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img = img.resize(size, Image.ANTIALIAS)\n",
    "                img.save(os.path.join(output_dir, image), img.format)\n",
    "        if(i + 1) % 10000 == 0:\n",
    "            print(\"[{}/{}] Image Resized\".format(i + 1, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXDiseyMw1kQ"
   },
   "outputs": [],
   "source": [
    "img_size = [32, 32]\n",
    "resize_images(FILE_PATH, './data', img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj8M0AjNftnB"
   },
   "outputs": [],
   "source": [
    "def tokenize(input_dir, output_path):\n",
    "    \"\"\"function for tokenizing and getting id for image\n",
    "\n",
    "    Args: \n",
    "        input_dir: directory path of which contains images\n",
    "        output_file: name of output file\n",
    "    \n",
    "    Returns:\n",
    "        output file\n",
    "    \n",
    "    Notes: \n",
    "        image name format: {class name(contains space)}_{location: int}_{image no: int}.jpg\n",
    "\n",
    "    \"\"\"\n",
    "    names = os.listdir(input_dir)\n",
    "    explain = dict()\n",
    "    full_names = dict()\n",
    "    for name in names[:5]:\n",
    "        tokens = name.split('.')[0]  # remove file formatter\n",
    "        tokens = tokens.split('_')\n",
    "        classes = []\n",
    "        id=\"\"\n",
    "        for token in tokens:\n",
    "            if(token.isdigit()):\n",
    "                id += token\n",
    "            else:\n",
    "                classes.append(token)\n",
    "        explain[id] = classes\n",
    "        full_names[id] = name\n",
    "    \n",
    "    with open(output_path + \"imgclass_map.json\", 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(explain, f)\n",
    "    with open(output_path + \"imgname_map.json\", 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(full_names, f)\n",
    "\n",
    "    return explain, full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WasBgS_enFuz"
   },
   "outputs": [],
   "source": [
    "explain, full_names = tokenize(FILE_PATH, './data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K149LhVatt3u",
    "outputId": "af03b531-f3c6-4e3d-a2d4-0cdc48d8d232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00403603': 'circular fish trap_004_03603.jpg', '00403579': 'circular fish trap_004_03579.jpg', '00403608': 'circular fish trap_004_03608.jpg', '00403639': 'circular fish trap_004_03639.jpg', '00403630': 'circular fish trap_004_03630.jpg'}\n"
     ]
    }
   ],
   "source": [
    "print(full_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhHlAHr9tx5X",
    "outputId": "dd4e3f47-9dfa-41bd-8a3f-aad094fe5126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00403603': ['circular fish trap'], '00403579': ['circular fish trap'], '00403608': ['circular fish trap'], '00403639': ['circular fish trap'], '00403630': ['circular fish trap']}\n"
     ]
    }
   ],
   "source": [
    "print(explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1fs8NpnPcDJ"
   },
   "source": [
    "- 클래스를 int로 변경\n",
    "- 변경한 결과를 json 파일로 저장\n",
    "- 추가로 그 클래스가 몇 개인지도 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozfx1clZMFuk"
   },
   "outputs": [],
   "source": [
    "class Debris(object):\n",
    "    def __init__(self, debris_file, img_mapped):\n",
    "        self.debris_file = debris_file\n",
    "        self.img_mapped = img_mapped\n",
    "        self.debris2idx = {}\n",
    "        self.idx2debris=[]\n",
    "        self.idx = 0\n",
    "        self.debris_cnt = Counter()\n",
    "\n",
    "        self.read_file()\n",
    "    \n",
    "    def read_file(self):\n",
    "        if(os.path.exists(self.debris_file)):\n",
    "            with open(self.debris_file, 'r') as f:\n",
    "                debris = json.load(f)\n",
    "                self.debris2idx = debris['debris2idx']\n",
    "                self.idx2debris = debris['idx2debris']\n",
    "                self.idx = len(self.idx2debris) + 1\n",
    "                self.debris_cnt = Counter(debris['debris_cnt'])\n",
    "        else:\n",
    "            self.build_debris()\n",
    "            with open(self.debris_file, 'w', encoding = 'utf-8') as f:\n",
    "                json.dump({\n",
    "                    'debris2idx': self.debris2idx,\n",
    "                    'idx2debris': self.idx2debris,\n",
    "                    'debris_cnt': self.debris_cnt\n",
    "                }, f)\n",
    "    \n",
    "    def add_debris(self, debris):\n",
    "        if not debris in self.debris2idx:\n",
    "            self.debris2idx[debris] = self.idx\n",
    "            self.idx2debris.append(debris)\n",
    "            self.idx += 1\n",
    "\n",
    "    def build_debris(self):\n",
    "        for _, debris in self.img_mapped.items():\n",
    "            for word in debris:\n",
    "                self.add_debris(word)\n",
    "            self.debris_cnt.update(debris)\n",
    "\n",
    "    def __call__(self, debris):\n",
    "        return self.debris2idx[debris]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.debris2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvlQU4xWoaf7"
   },
   "outputs": [],
   "source": [
    "class MarinTrashDS(Dataset):\n",
    "    def __init__(self, explain, full_names, debris_file, trasform=None):\n",
    "        self.debris = Debris(debris_file = debris_file, img_mapped=explain)\n",
    "        self.transform = transform\n",
    "        self.img_index =  [ele for ele in explain]\n",
    "        self.num_classes = len(self.debris)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_index[index]\n",
    "        classes = explain[img_id]\n",
    "        target = np.zeros(self.num_classes, dtype = np.float)\n",
    "        for word in classes:           \n",
    "            int_target = self.debris(word)\n",
    "            target[int_target] = 1.0\n",
    "        target = torch.Tensor(target)\n",
    "\n",
    "        path = './data/' + full_names[img_id]        \n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None: \n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEeBaWdvviY7"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = 0.5, std = 0.5)                                \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5RCd47SxbRl"
   },
   "outputs": [],
   "source": [
    "train_dataset = MarinTrashDS(explain = explain, full_names = full_names, debris_file = './data/debris.json', trasform = transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXbdwXJw9YRD"
   },
   "outputs": [],
   "source": [
    "tr_index, val_index = train_test_split(list(range(len(train_dataset))), test_size = 0.2, shuffle = True)\n",
    "tr_sampler = SubsetRandomSampler(tr_index)\n",
    "val_sampler = SubsetRandomSampler(val_index)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AI_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
