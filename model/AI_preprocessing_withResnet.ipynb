{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxhJXClZfask"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wul_jMrMfel0"
   },
   "source": [
    "## resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VW8AeCgBmwoM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "# from google.colab import drive\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import Counter\n",
    "import math\n",
    "from torch.nn import BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6RCbCFNm1Wq",
    "outputId": "ea33aa03-8916-4314-cdeb-40cc4141fd4b"
   },
   "outputs": [],
   "source": [
    "# drive.mount('/content/drive')\n",
    "FILE_PATH =  \"C:\\\\Users\\\\dipreez\\\\Downloads\\\\해양 침적 쓰레기 이미지\\\\Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gcU10E3GnTvE"
   },
   "outputs": [],
   "source": [
    "os.chdir(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Kltu8mtjfZM9"
   },
   "outputs": [],
   "source": [
    "def resize_images(image_dir, output_dir, size):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    images = os.listdir(image_dir)\n",
    "    num_images = len(images)\n",
    "    \n",
    "\n",
    "    for i, image in enumerate(images[:5]):\n",
    "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img = img.resize(size, Image.ANTIALIAS)\n",
    "                img.save(os.path.join(output_dir, image), img.format)\n",
    "        if(i + 1) % 10000 == 0:\n",
    "            print(\"[{}/{}] Image Resized\".format(i + 1, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mXDiseyMw1kQ"
   },
   "outputs": [],
   "source": [
    "img_size = [64, 64]\n",
    "resize_images(FILE_PATH, './data', img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Xj8M0AjNftnB"
   },
   "outputs": [],
   "source": [
    "def tokenize(input_dir, output_path):\n",
    "    \"\"\"function for tokenizing and getting id for image\n",
    "\n",
    "    Args: \n",
    "        input_dir: directory path of which contains images\n",
    "        output_file: name of output file\n",
    "    \n",
    "    Returns:\n",
    "        output file\n",
    "    \n",
    "    Notes: \n",
    "        image name format: {class name(contains space)}_{location: int}_{image no: int}.jpg\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: class를 int로 변경 & 저장\n",
    "    names = os.listdir(input_dir)\n",
    "    explain = dict()\n",
    "    full_names = dict()\n",
    "    for name in names[:5]:\n",
    "        tokens = name.split('.')[0]  # remove file formatter\n",
    "        tokens = tokens.split('_')\n",
    "        classes = []\n",
    "        id=\"\"\n",
    "        for token in tokens:\n",
    "            if(token.isdigit()):\n",
    "                id += token\n",
    "            else:\n",
    "                classes.append(token)\n",
    "        explain[id] = classes\n",
    "        full_names[id] = name\n",
    "    \n",
    "    with open(output_path + \"imgclass_map.json\", 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(explain, f)\n",
    "    with open(output_path + \"imgname_map.json\", 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(full_names, f)\n",
    "\n",
    "    return explain, full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WasBgS_enFuz"
   },
   "outputs": [],
   "source": [
    "explain, full_names = tokenize(FILE_PATH, './data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhHlAHr9tx5X",
    "outputId": "dd4e3f47-9dfa-41bd-8a3f-aad094fe5126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00200012': 'bundle of ropes_002_00012.jpg', '00200039': 'bundle of ropes_002_00039.jpg', '00200084': 'bundle of ropes_002_00084.jpg', '00200147': 'bundle of ropes_002_00147.jpg', '00200156': 'bundle of ropes_002_00156.jpg'}\n"
     ]
    }
   ],
   "source": [
    "print(full_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1fs8NpnPcDJ"
   },
   "source": [
    "- 클래스를 int로 변경\n",
    "- 변경한 결과를 json 파일로 저장\n",
    "- 추가로 그 클래스가 몇 개인지도 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ozfx1clZMFuk"
   },
   "outputs": [],
   "source": [
    "class Debris(object):\n",
    "    def __init__(self, debris_file, img_mapped):\n",
    "        self.debris_file = debris_file\n",
    "        self.img_mapped = img_mapped\n",
    "        self.debris2idx = {}\n",
    "        self.idx2debris=[]\n",
    "        self.idx = 0\n",
    "        self.debris_cnt = Counter()\n",
    "\n",
    "        self.read_file()\n",
    "    \n",
    "    def read_file(self):\n",
    "        if(os.path.exists(self.debris_file)):\n",
    "            with open(self.debris_file, 'r') as f:\n",
    "                debris = json.load(f)\n",
    "                self.debris2idx = debris['debris2idx']\n",
    "                self.idx2debris = debris['idx2debris']\n",
    "                self.idx = len(self.idx2debris) + 1\n",
    "                self.debris_cnt = Counter(debris['debris_cnt'])\n",
    "        else:\n",
    "            self.build_debris()\n",
    "            with open(self.debris_file, 'w', encoding = 'utf-8') as f:\n",
    "                json.dump({\n",
    "                    'debris2idx': self.debris2idx,\n",
    "                    'idx2debris': self.idx2debris,\n",
    "                    'debris_cnt': self.debris_cnt\n",
    "                }, f)\n",
    "    \n",
    "    def add_debris(self, debris):\n",
    "        if not debris in self.debris2idx:\n",
    "            self.debris2idx[debris] = self.idx\n",
    "            self.idx2debris.append(debris)\n",
    "            self.idx += 1\n",
    "\n",
    "    def build_debris(self):\n",
    "        for _, debris in self.img_mapped.items():\n",
    "            for word in debris:\n",
    "                self.add_debris(word)\n",
    "            self.debris_cnt.update(debris)\n",
    "\n",
    "    def __call__(self, debris):\n",
    "        return self.debris2idx[debris]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.debris2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MvlQU4xWoaf7"
   },
   "outputs": [],
   "source": [
    "class MarinTrashDS(Dataset):\n",
    "    def __init__(self, explain, full_names, debris_file, trasform=None):\n",
    "        self.debris = Debris(debris_file = debris_file, img_mapped=explain)\n",
    "        self.transform = transform\n",
    "        self.img_index =  [ele for ele in explain]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_index[index]\n",
    "        classes = explain[img_id]\n",
    "        target = []\n",
    "        for word in classes:           \n",
    "            target.append(self.debris(word))\n",
    "        target = torch.Tensor(target).long()\n",
    "\n",
    "        path = './data/' + full_names[img_id]        \n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None: \n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GEeBaWdvviY7"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = 0.5, std = 0.5)                                \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "A5RCd47SxbRl"
   },
   "outputs": [],
   "source": [
    "train_dataset = MarinTrashDS(explain = explain, full_names = full_names, debris_file = './data/debris.json', trasform = transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GXbdwXJw9YRD"
   },
   "outputs": [],
   "source": [
    "tr_index, val_index = train_test_split(list(range(len(train_dataset))), test_size = 0.2, shuffle = True)\n",
    "tr_sampler = SubsetRandomSampler(tr_index)\n",
    "val_sampler = SubsetRandomSampler(val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AjS-pIoN34Ku"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 32, num_workers=0, sampler = tr_sampler)\n",
    "val_loader = DataLoader(train_dataset, batch_size = 32, num_workers = 0, sampler = val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ffDlipgq6V3q"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample=downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion, stride)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        ########################################\n",
    "        num_classes: int = 1,\n",
    "        ########################################\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        ########################################\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        ########################################\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample=None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet(BasicBlock, [2, 2, 2, 2]).to(device)\n",
    "\n",
    "criterion = BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total = len(tr_index)\n",
    "valid_total = len(val_index)\n",
    "\n",
    "train_batches = len(train_loader)\n",
    "valid_batches = len(val_loader)\n",
    "\n",
    "# Variables for lr scheduling and early stopping\n",
    "best_valid_loss = 1024    # Any large number will suffice\n",
    "patience = 0    # Bad epoch counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1]) torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dipreez\\AppData\\Local\\Temp/ipykernel_32792/4236042798.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at C:/cb/pytorch_1000000000000/work/aten/src\\THCUNN/generic/ClassNLLCriterion.cu:15",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32792/4236042798.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1121\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2822\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2824\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: multi-target not supported at C:/cb/pytorch_1000000000000/work/aten/src\\THCUNN/generic/ClassNLLCriterion.cu:15"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "epochs =100\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Train\n",
    "    net.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "        outputs = net(x)\n",
    "        print(outputs.shape,y.shape)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_correct += predicted.eq(y).sum().item()\n",
    "        \n",
    "    train_loss = train_loss / train_batches\n",
    "    train_acc = train_correct / train_total\n",
    "    \n",
    "    # Validate\n",
    "    net.eval()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            x = x.to(device)\n",
    "            y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "            outputs = net(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            valid_correct += predicted.eq(y).sum().item()\n",
    "            \n",
    "    valid_loss = valid_loss / valid_batches\n",
    "    valid_acc = valid_correct / valid_total\n",
    "    \n",
    "    # Save best model\n",
    "    if best_valid_loss > valid_loss:\n",
    "        torch.save(net.state_dict(), './2018310773_임승재.pt')\n",
    "        best_valid_loss = valid_loss\n",
    "        patience = 0\n",
    "\n",
    "    if epoch%(epochs/10) == 0:\n",
    "      print('[%d/%d] TrainLoss: %.3f, ValLoss: %.3f | TrainAcc: %.2f, ValAcc: %.2f' % (epoch, epochs, train_loss, valid_loss, train_acc, valid_acc))\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Break training loop if no improvement for 10 consecutive epochs\n",
    "    if patience == 10:\n",
    "        print('No improvement found. Escaped for loop at [%d/%d]' % (epoch,epochs))\n",
    "        break\n",
    "        \n",
    "    patience += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AI_preprocessing_withTest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
